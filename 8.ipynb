{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dadec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://github.com/alexeygrigorev/dino-or-dragon/releases/download/data/dino-dragon.zip\n",
    "# !unzip dino-dragon.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e7fc650",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 23:37:12.082739: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-21 23:37:12.172893: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-21 23:37:12.175406: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-21 23:37:12.175422: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-21 23:37:12.630372: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-21 23:37:12.630421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-21 23:37:12.630425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65ee19",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "The dataset contains around 1900 images of dinos and around 1900 images of dragons.\n",
    "\n",
    "The dataset contains separate folders for training and test sets.\n",
    "Model\n",
    "\n",
    "For this homework we will use Convolutional Neural Network (CNN). Like in the lectures, we'll use Keras.\n",
    "\n",
    "You need to develop the model with following structure:\n",
    "\n",
    "    The shape for input should be (150, 150, 3)\n",
    "    Next, create a convolutional layer (Conv2D):\n",
    "        Use 32 filters\n",
    "        Kernel size should be (3, 3) (that's the size of the filter)\n",
    "        Use 'relu' as activation\n",
    "    Reduce the size of the feature map with max pooling (MaxPooling2D)\n",
    "        Set the pooling size to (2, 2)\n",
    "    Turn the multi-dimensional result into vectors using a Flatten layer\n",
    "    Next, add a Dense layer with 64 neurons and 'relu' activation\n",
    "    Finally, create the Dense layer with 1 neuron - this will be the output\n",
    "        The output layer should have an activation - use the appropriate activation for the binary classification case\n",
    "\n",
    "As optimizer use SGD with the following parameters:\n",
    "\n",
    "    SGD(lr=0.002, momentum=0.8)\n",
    "\n",
    "For clarification about kernel size and max pooling, check Office Hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc06c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 23:37:13.169492: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-11-21 23:37:13.169512: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-11-21 23:37:13.169541: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (3511): /proc/driver/nvidia/version does not exist\n",
      "2022-11-21 23:37:13.169719: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9916eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=SGD(learning_rate=0.002, momentum=0.8),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f1d38a",
   "metadata": {},
   "source": [
    "#### Q1. Since we have a binary classification problem, what is the best loss function for us?\n",
    "\n",
    "Note: since we specify an activation for the output layer, we don't need to set from_logits=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9b56e5",
   "metadata": {},
   "source": [
    "#### binary crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04589b70",
   "metadata": {},
   "source": [
    "#### Q2. What's the total number of parameters of the model? You can use the summary method for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7befeec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 148, 148, 32)      896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 74, 74, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 175232)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                11214912  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,215,873\n",
      "Trainable params: 11,215,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbee213",
   "metadata": {},
   "source": [
    "#### 11215873"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649200f",
   "metadata": {},
   "source": [
    "Generators and Training\n",
    "\n",
    "For the next two questions, use the following data generator for both train and test sets:\n",
    "\n",
    "ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    We don't need to do any additional pre-processing for the images.\n",
    "    When reading the data from train/val directories, check the class_mode parameter. Which value should it be for a binary classification problem?\n",
    "    Use batch_size=20\n",
    "    Use shuffle=True for both training and test sets.\n",
    "\n",
    "For training use .fit() with the following params:\n",
    "\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,\n",
    "    validation_data=test_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00467877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1594 images belonging to 2 classes.\n",
      "Found 394 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "image_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_gen = image_gen.flow_from_directory(\n",
    "    directory='./train/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    ")\n",
    "\n",
    "test_gen = image_gen.flow_from_directory(\n",
    "    directory='./test/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff408e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 9s 109ms/step - loss: 0.6812 - accuracy: 0.6248 - val_loss: 0.6327 - val_accuracy: 0.5838\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 0.5463 - accuracy: 0.7491 - val_loss: 0.6558 - val_accuracy: 0.5787\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 9s 111ms/step - loss: 0.4554 - accuracy: 0.8011 - val_loss: 0.4071 - val_accuracy: 0.8503\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 0.4026 - accuracy: 0.8356 - val_loss: 0.3845 - val_accuracy: 0.8350\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 9s 114ms/step - loss: 0.3499 - accuracy: 0.8532 - val_loss: 0.3546 - val_accuracy: 0.8553\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.3427 - accuracy: 0.8582 - val_loss: 0.3584 - val_accuracy: 0.8477\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.2877 - accuracy: 0.8902 - val_loss: 0.5510 - val_accuracy: 0.7614\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 9s 117ms/step - loss: 0.2651 - accuracy: 0.9034 - val_loss: 0.3047 - val_accuracy: 0.8706\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 10s 120ms/step - loss: 0.2397 - accuracy: 0.9072 - val_loss: 0.3019 - val_accuracy: 0.8629\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 9s 118ms/step - loss: 0.2151 - accuracy: 0.9178 - val_loss: 0.3405 - val_accuracy: 0.8452\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=test_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fc7b1a",
   "metadata": {},
   "source": [
    "#### Q3. What is the median of training accuracy for all the epochs for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a763e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.median(history.history['accuracy']), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fee76f9",
   "metadata": {},
   "source": [
    "#### 0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fd52f4",
   "metadata": {},
   "source": [
    "#### Q4. What is the standard deviation of training loss for all the epochs for this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8397a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.std(history.history['loss']), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83293a0c",
   "metadata": {},
   "source": [
    "#### 0.11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baca28e1",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "\n",
    "For the next two questions, we'll generate more data using data augmentations.\n",
    "\n",
    "Add the following augmentations to your training data generator:\n",
    "\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9e6399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1594 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_gen = train_gen.flow_from_directory(\n",
    "    directory='./train/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=20,\n",
    "    shuffle=True,\n",
    "    class_mode='binary',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d72a67",
   "metadata": {},
   "source": [
    "Let's train our model for 10 more epochs using the same code as previously. Make sure you don't re-create the model - we want to continue training the model we already started training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16e42302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 14s 171ms/step - loss: 0.4928 - accuracy: 0.7654 - val_loss: 0.3431 - val_accuracy: 0.8503\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 14s 174ms/step - loss: 0.4126 - accuracy: 0.8187 - val_loss: 0.3210 - val_accuracy: 0.8731\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 14s 177ms/step - loss: 0.4201 - accuracy: 0.8024 - val_loss: 0.4033 - val_accuracy: 0.8147\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 14s 173ms/step - loss: 0.4106 - accuracy: 0.8162 - val_loss: 0.3441 - val_accuracy: 0.8376\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 14s 174ms/step - loss: 0.4047 - accuracy: 0.8130 - val_loss: 0.4860 - val_accuracy: 0.7640\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 14s 175ms/step - loss: 0.3793 - accuracy: 0.8262 - val_loss: 0.3917 - val_accuracy: 0.8249\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 14s 174ms/step - loss: 0.3841 - accuracy: 0.8325 - val_loss: 0.2909 - val_accuracy: 0.8706\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 14s 176ms/step - loss: 0.3621 - accuracy: 0.8444 - val_loss: 0.5480 - val_accuracy: 0.7614\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 14s 175ms/step - loss: 0.3507 - accuracy: 0.8413 - val_loss: 0.4759 - val_accuracy: 0.7792\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 14s 174ms/step - loss: 0.3465 - accuracy: 0.8469 - val_loss: 0.3094 - val_accuracy: 0.8477\n"
     ]
    }
   ],
   "source": [
    "history2 = model.fit(\n",
    "    train_gen,\n",
    "    epochs=10,\n",
    "    validation_data=test_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbc8b7",
   "metadata": {},
   "source": [
    "#### Q5. What is the mean of test loss for all the epochs for the model trained with augmentations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0baa2174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(history2.history['loss']), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3fc5ea",
   "metadata": {},
   "source": [
    "#### 0.37"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcc2d79",
   "metadata": {},
   "source": [
    "#### Q6. What's the average of test accuracy for the last 5 epochs (from 6 to 10) for the model trained with augmentations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "595e1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean(history2.history['accuracy'][5:10]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8986e6",
   "metadata": {},
   "source": [
    "#### 0.84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df0038f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
